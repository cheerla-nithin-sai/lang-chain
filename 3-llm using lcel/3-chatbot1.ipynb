{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## developing a chatbot using opensource model gemma-2b\n",
    "## using message history\n",
    "# This chatbot will be able to have a conversation and remember previous interactions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# to load env variables\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatGroq(client=<groq.resources.chat.completions.Completions object at 0x0000017E996A6210>, async_client=<groq.resources.chat.completions.AsyncCompletions object at 0x0000017E9964D210>, model_name='Gemma2-9b-It', model_kwargs={}, groq_api_key=SecretStr('**********'))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# setting up goq api and model\n",
    "from langchain_groq import ChatGroq\n",
    "groq_api = os.getenv('GROQ_API_KEY')\n",
    "llm_1 = ChatGroq(model='Gemma2-9b-It',groq_api_key=groq_api)\n",
    "llm_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"Hi Nithin! It's nice to meet you. \\n\\nWhat kind of student are you? What are you studying? 😊 \\n\\n\", additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 32, 'prompt_tokens': 20, 'total_tokens': 52, 'completion_time': 0.058181818, 'prompt_time': 7.858e-05, 'queue_time': 0.013484859, 'total_time': 0.058260398}, 'model_name': 'Gemma2-9b-It', 'system_fingerprint': 'fp_10c08bf97d', 'finish_reason': 'stop', 'logprobs': None}, id='run-3579c217-d58f-4466-b38f-e4b9ac86a250-0', usage_metadata={'input_tokens': 20, 'output_tokens': 32, 'total_tokens': 52})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# here AImesaage is used to store the ai message given by model itself to store the info, if not given then model can,t remember the previous given output\n",
    "from langchain_core.messages import HumanMessage,AIMessage,SystemMessage\n",
    "\n",
    "llm_1.invoke([HumanMessage(content=\"Hi my name is nithin and I am a student\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"You are Nithin, a student!  \\n\\nIt seems like you're curious about yourself.  \\n\\nWhat else would you like to know about who you are?  Perhaps you could tell me about your hobbies, your favorite subjects, or your dreams for the future. 😊 \\n\\n\", additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 61, 'prompt_tokens': 65, 'total_tokens': 126, 'completion_time': 0.110909091, 'prompt_time': 0.001971253, 'queue_time': 0.011430515999999998, 'total_time': 0.112880344}, 'model_name': 'Gemma2-9b-It', 'system_fingerprint': 'fp_10c08bf97d', 'finish_reason': 'stop', 'logprobs': None}, id='run-44765ff7-e44f-4438-9df0-593564d6c829-0', usage_metadata={'input_tokens': 65, 'output_tokens': 61, 'total_tokens': 126})"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# above we got an output in AIMessage now, we need to feed the model with same message to remember\n",
    "messages = (\n",
    "    [\n",
    "        HumanMessage(content=\"Hi my name is nithin and I am a student\"),\n",
    "        AIMessage(content=\"Hi Nithin! It's nice to meet you. \\n\\nWhat kind of student are you? What are you studying?\"), # feeding model with ai message\n",
    "        HumanMessage(content=\"what is my name and who am I?\") # checking weather model remembers my name\n",
    "    ])\n",
    "\n",
    "llm_1.invoke(messages)\n",
    "# we cannot feed all the time to model so we use message history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Message History\n",
    "#### This will keep track of inputs and outputs of the model, and store them in some datastore. Future interactions will then load those messages and pass them into the chain as part of the input. Let's see how to use this!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.chat_history import BaseChatMessageHistory,InMemoryChatMessageHistory\n",
    "from langchain_community.chat_message_histories import ChatMessageHistory  # Stores messages in an in memory list.\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "\n",
    "store_sessions ={} # dict to store the sessions\n",
    "\n",
    "# func to take session id and give the message history of that session\n",
    "def get_session_history(session_id:str)->BaseChatMessageHistory:\n",
    "    if session_id not in store_sessions:\n",
    "        store_sessions[session_id] = ChatMessageHistory()\n",
    "    return store_sessions[session_id]\n",
    "\n",
    "# runnaable with message history is used to interact with the llm model\n",
    "with_message_history = RunnableWithMessageHistory(llm_1,get_session_history)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Hi Nithin! It's nice to meet you.  \\n\\nWhat kind of data science work do you enjoy doing?  \\n\\nI'm still learning about the field, so I'm always interested to hear about what people are working on. 😊 \\n\\n\""
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We now need to create a config that we pass into the runnable every time\n",
    "config = {\"configurable\": {\"session_id\": \"chat1\"}}\n",
    "\n",
    "response = with_message_history.invoke(\n",
    "    [HumanMessage(content=\"Hi my name is nithin and I an a DataScientist\")],\n",
    "    config = config)\n",
    "\n",
    "response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Your name is Nithin.  \\n\\nI remember that from our earlier conversation.  How can I help you today? 😊 \\n', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 29, 'prompt_tokens': 90, 'total_tokens': 119, 'completion_time': 0.052727273, 'prompt_time': 0.002417722, 'queue_time': 0.012188967, 'total_time': 0.055144995}, 'model_name': 'Gemma2-9b-It', 'system_fingerprint': 'fp_10c08bf97d', 'finish_reason': 'stop', 'logprobs': None}, id='run-37f832fb-ed84-4f31-b5f6-9b5d85d63e5e-0', usage_metadata={'input_tokens': 90, 'output_tokens': 29, 'total_tokens': 119})"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# checking the model\n",
    "with_message_history.invoke(\n",
    "    [HumanMessage(content=\"WHAT is my name?\")],\n",
    "    config = config\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I don't know your name.  \\n\\nI'm a large language model, so I don't have any memory of past conversations. If you'd like to tell me your name, I'd be happy to learn it! 😊 \\n\""
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# if we change the config(session_id) and try to get the same\n",
    "config1 ={'configurable':{\"session_id\":\"chat2\"}}\n",
    "\n",
    "response_2  = with_message_history.invoke(\n",
    "    [HumanMessage(content=\"What is my name?\")],\n",
    "    config = config1\n",
    ")\n",
    "response_2.content\n",
    "\n",
    "# as new session is created model doesnt have previous session data\n",
    "# we need to again feed the model with my name in diff session id and then try to get my name again"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using Prompt Template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate,MessagesPlaceholder \n",
    "# message place holder to pass all the messages in.\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\",\"you are helpful assitant. answer all the questions to your best ability\"),\n",
    "        MessagesPlaceholder(variable_name=\"messages\") # what ever human messsage we want to give we have to give it to key-value pair with key=\"messages\"\n",
    "    ])\n",
    "\n",
    "chain = prompt | llm_1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"Hello Nithin! It's nice to meet you. \\n\\nIs there anything I can help you with today?  I'm ready for your questions! 😊 \\n\", additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 38, 'prompt_tokens': 29, 'total_tokens': 67, 'completion_time': 0.069090909, 'prompt_time': 0.00023236, 'queue_time': 0.012832860000000001, 'total_time': 0.069323269}, 'model_name': 'Gemma2-9b-It', 'system_fingerprint': 'fp_10c08bf97d', 'finish_reason': 'stop', 'logprobs': None}, id='run-2750aaa9-7174-48d5-aabe-ffed3b758145-0', usage_metadata={'input_tokens': 29, 'output_tokens': 38, 'total_tokens': 67})"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# after chaining to get the output\n",
    "\n",
    "chain.invoke({\"messages\":[HumanMessage(content=\"Hi my name is nithin\")]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Hi Nithin!  It's nice to finally meet you.  \\n\\nHow can I be helpful today? 😊  \\n\\n\""
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We can now wrap this in the same Messages History object as before\n",
    "with_message_history_2 = RunnableWithMessageHistory(chain,get_session_history)\n",
    "config2 = {\"configurable\":{\"session_id\":\"chat3\"}}\n",
    "response_3 = with_message_history_2.invoke(\n",
    "    [HumanMessage(content=\"Hi my name is Nithin\")],\n",
    "    config = config2)\n",
    "response_3.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Your name is Nithin!  😊 \\n\\nIs there anything else I can help you with?  \\n', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 24, 'prompt_tokens': 154, 'total_tokens': 178, 'completion_time': 0.043636364, 'prompt_time': 0.005883175, 'queue_time': 0.008483972, 'total_time': 0.049519539}, 'model_name': 'Gemma2-9b-It', 'system_fingerprint': 'fp_10c08bf97d', 'finish_reason': 'stop', 'logprobs': None}, id='run-230a9a8f-3bc9-4d16-8981-e841e942ce49-0', usage_metadata={'input_tokens': 154, 'output_tokens': 24, 'total_tokens': 178})"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# to get the output from above config\n",
    "with_message_history_2.invoke([HumanMessage(content=\"What is my name\")],config=config2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='మీ పేరు నితిన్ అని తెలుసుకున్నాను! 😊   \\n\\nనేను మీకు సహాయం చేయడానికి ఇక్కడ ఉన్నాను. ఏదైనా ప్రశ్న ఉంటే, నేను నా శక్తికి సరిపడా జవాబు ఇస్తాను. \\n\\n', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 79, 'prompt_tokens': 31, 'total_tokens': 110, 'completion_time': 0.143636364, 'prompt_time': 0.000369088, 'queue_time': 0.014875311, 'total_time': 0.144005452}, 'model_name': 'Gemma2-9b-It', 'system_fingerprint': 'fp_10c08bf97d', 'finish_reason': 'stop', 'logprobs': None}, id='run-b4a271b3-c33b-43a3-a4da-98104d06ae86-0', usage_metadata={'input_tokens': 31, 'output_tokens': 79, 'total_tokens': 110})"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we can complicate the prompt as well by adding extra variables\n",
    "prompt2 = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\",\"you are helpful assitant. answer all the questions to your best ability in {language}\"),\n",
    "        MessagesPlaceholder(variable_name=\"messages\")\n",
    "    ]\n",
    ")\n",
    "chain2 = prompt2 | llm_1\n",
    "\n",
    "chain2.invoke({\"messages\":[HumanMessage(content=\"Hi my name is nithin\")],\"language\":\"Telugu\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"That's a great name, Nithinsai!  😊  It's nice to officially know your full name.  \\n\\nIs there anything else I can help you with?\\n\""
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# as we have multiple keys in input(messages,language) we are specifing the messages key\n",
    "with_message_history_3 = RunnableWithMessageHistory(chain2,get_session_history,input_messages_key=\"messages\")\n",
    "config3 = {\"configurable\":{\"session_id\":\"chat4\"}}\n",
    "\n",
    "response_4 = with_message_history_3.invoke({\"messages\":[HumanMessage(content=\"my name is nithinsai\")],\"language\":\"Hindi\"},config=config3)\n",
    "response_4.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Managing Converstion History\n",
    "#### One important concept to understand when building chatbots is how to manage conversation history. If left unmanaged, the list of messages will grow unbounded and potentially overflow the context window of the LLM. Therefore, it is important to add a step that limits the size of the messages you are passing in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SystemMessage(content=\"you're a good assistant\", additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content=\"hi! I'm Nithin\", additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='hi!', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='I like vanilla ice cream', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='nice', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='whats 2 + 2', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='4', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='thanks', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='no problem!', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='having fun?', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='yes!', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='I love reading fiction books', additional_kwargs={}, response_metadata={})]"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.messages import trim_messages,SystemMessage\n",
    "#  trim_messages helper to reduce how many messages we're sending to the model. The trimmer allows us to specify how many tokens we want to keep\n",
    "\n",
    "trimmer = trim_messages(\n",
    "    max_tokens=70,\n",
    "    strategy=\"last\",\n",
    "    token_counter=llm_1, # counter is llm model\n",
    "    include_system=True, # include the system message\n",
    "    allow_partial=False,\n",
    "    start_on=\"human\",\n",
    ")\n",
    "messages = [\n",
    "    SystemMessage(content=\"you're a good assistant\"),\n",
    "    HumanMessage(content=\"hi! I'm Nithin\"),\n",
    "    AIMessage(content=\"hi!\"),\n",
    "    HumanMessage(content=\"I like vanilla ice cream\"),\n",
    "    AIMessage(content=\"nice\"),\n",
    "    HumanMessage(content=\"whats 2 + 2\"),\n",
    "    AIMessage(content=\"4\"),\n",
    "    HumanMessage(content=\"thanks\"),\n",
    "    AIMessage(content=\"no problem!\"),\n",
    "    HumanMessage(content=\"having fun?\"),\n",
    "    AIMessage(content=\"yes!\"),\n",
    "    HumanMessage(content=\"I love reading fiction books\")\n",
    "]\n",
    "\n",
    "trimmer.invoke(messages)\n",
    "\n",
    "# here the trimmer only considered 40 tokens from last so, except sys message: first 4 messages are trimmed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Importantly, you will want to do this BEFORE the prompt template but AFTER you load previous messages from Message History.\n",
    "# To use it in our chain, we just need to run the trimmer before we pass the messages input to our prompt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Your name is Nithin 😊 \\n', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 10, 'prompt_tokens': 133, 'total_tokens': 143, 'completion_time': 0.018181818, 'prompt_time': 0.00528705, 'queue_time': 0.007858489, 'total_time': 0.023468868}, 'model_name': 'Gemma2-9b-It', 'system_fingerprint': 'fp_10c08bf97d', 'finish_reason': 'stop', 'logprobs': None}, id='run-ab9c9501-e201-452b-a68b-78181e087f3d-0', usage_metadata={'input_tokens': 133, 'output_tokens': 10, 'total_tokens': 143})"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from operator import itemgetter\n",
    "\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "# Runnable to passthrough inputs unchanged or with additional keys.\n",
    "# rpt.assign is to add or modify data in the input dictionary by specifying keyword arguments.\n",
    "\n",
    "chain3 = (RunnablePassthrough.assign(messages=itemgetter(\"messages\") | trimmer) | prompt2 | llm_1)\n",
    "\n",
    "chain3.invoke({\"messages\":messages+[HumanMessage(content=\"what is my name?\")],\n",
    "               \"language\":\"English\"})\n",
    "\n",
    "# in above although i have given less tokens name will get trimmed and model is not able to capture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='You asked \"What\\'s 2 + 2?\"  😊 \\n\\nDo you have another one for me? \\n', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 28, 'prompt_tokens': 115, 'total_tokens': 143, 'completion_time': 0.050909091, 'prompt_time': 0.003798866, 'queue_time': 0.011754673, 'total_time': 0.054707957}, 'model_name': 'Gemma2-9b-It', 'system_fingerprint': 'fp_10c08bf97d', 'finish_reason': 'stop', 'logprobs': None}, id='run-57a2d3c5-f469-4fe2-9487-db35ce337d61-0', usage_metadata={'input_tokens': 115, 'output_tokens': 28, 'total_tokens': 143})"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " # if we ask any question from untrimmed messages model should give output\n",
    "chain3.invoke({\"messages\":messages+[HumanMessage(content=\"what math problem did i ask\")],\n",
    "               \"language\":\"English\"})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Hi Nithin! It's nice to meet you.  \\n\\nWhat kind of data science work do you enjoy doing?  \\n\\nI'm still learning about the field, so I'm always interested to hear about what people are working on. 😊 \\n\\n\""
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#  wrap this in the Message History\n",
    "with_message_history_4 = RunnableWithMessageHistory(chain3,get_session_history,input_messages_key=\"messages\")\n",
    "\n",
    "config4 = {\"configurable\": {\"session_id\": \"chat5\"}}\n",
    "\n",
    "response_5 = with_message_history_4.invoke(\n",
    "    {\n",
    "        \"messages\": messages + [HumanMessage(content=\"what is my name?\")],\n",
    "        \"language\": \"English\",\n",
    "    },\n",
    "    config=config4,\n",
    ")\n",
    "\n",
    "response.content\n",
    "# name got trimmed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"It's great to meet you! As an AI, I don't have personal feelings or experiences like love.  \\n\\nTo figure out what *you* love, maybe think about:\\n\\n* **What makes you feel happy and fulfilled?**\\n* **What do you enjoy spending your time doing?**\\n* **What are you passionate about?**\\n* **What are you naturally good at?**\\n\\nExploring these questions can help you discover what brings you joy and makes you feel truly alive! 😊  \\n\\n\", additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 110, 'prompt_tokens': 37, 'total_tokens': 147, 'completion_time': 0.2, 'prompt_time': 0.000356499, 'queue_time': 0.01395501, 'total_time': 0.200356499}, 'model_name': 'Gemma2-9b-It', 'system_fingerprint': 'fp_10c08bf97d', 'finish_reason': 'stop', 'logprobs': None}, id='run-5785345b-cc1e-48cb-886f-4dfe41edf651-0', usage_metadata={'input_tokens': 37, 'output_tokens': 110, 'total_tokens': 147})"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with_message_history_4.invoke(\n",
    "    {\n",
    "        \"messages\": [HumanMessage(content=\"what do I love?\")],\n",
    "        \"language\": \"English\",\n",
    "    },\n",
    "    config=config4,\n",
    ")\n",
    "# model still have info about last human messages\n",
    "# if you re-run this cell again and the last info is also gone\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
