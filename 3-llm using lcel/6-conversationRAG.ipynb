{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conversation Q/A RAG \n",
    "1. simple RAG app doestn't focus on previous questions(no chat memory )\n",
    "2. convo RAG has some sort of \"memory\" of past questions and answers, and some logic for incorporating those into its current thinking.\n",
    "\n",
    "- focusing on adding logic for incorporating historical messages using 2 approaches-->1. chains, 2. Agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# to load envi variables\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using groq for llm \n",
    "\n",
    "groq_api_key = os.getenv(\"GROQ_API_KEY\")\n",
    "from langchain_groq import ChatGroq\n",
    "llm = ChatGroq(model=\"llama3-8b-8192\",groq_api_key=groq_api_key,max_retries=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Admin\\Desktop\\Langchain\\venv\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# importing modules\n",
    "import bs4 # web scraping\n",
    "from langchain_community.document_loaders import WebBaseLoader # to load webpage content\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter # to split text \n",
    "from langchain_chroma import Chroma # to store vectors in db\n",
    "from langchain_core.prompts import ChatPromptTemplate # to define prompt for llm\n",
    "from langchain.chains import create_retrieval_chain #to create retrieval chain \n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain # Create a chain for passing a list of Documents to a model.\n",
    "\n",
    "# for free embeddings using hugging face models\n",
    "os.environ['HF_TOKEN']=os.getenv(\"HF_TOKEN\")\n",
    "from langchain_huggingface import HuggingFaceEmbeddings # to embed text to vectors\n",
    "HF_embedding = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Load, chunk and index the contents of the blog to create a retriever.\n",
    "loader = WebBaseLoader(\n",
    "    web_paths=(\"https://medium.com/@prateekgaurav/nlp-zero-to-hero-part-1-introduction-bow-tf-idf-word2vec-c1b11ed77a2#:~:text=Word2Vec%20vs.,in%20a%20dense%20vector%20space.\",),\n",
    "        bs_kwargs=dict(\n",
    "        parse_only=bs4.SoupStrainer(\n",
    "            class_=(\"post-content\", \"post-title\",\"post-header\")\n",
    "        )\n",
    "    ),\n",
    ")\n",
    "docs = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split embed and vectorspace\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000,chunk_overlap=200)\n",
    "\n",
    "splitted_docs = text_splitter.split_documents(docs)\n",
    "\n",
    "vectorstore = Chroma.from_documents(documents=docs,embedding=HF_embedding)\n",
    "\n",
    "retriever = vectorstore.as_retriever()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompts\n",
    "# system prompt is telling llm how should it act/work\n",
    "system_prompt = (\n",
    "    \"You are an assistant for question-answering tasks. \"\n",
    "    \"Use the following pieces of retrieved context to answer \"\n",
    "    \"the question. If you don't know the answer, say that you \"\n",
    "    \"don't know. Use three sentences maximum and keep the \"\n",
    "    \"answer concise.\"\n",
    "    \"\\n\\n\"\n",
    "    \"{context}\"\n",
    ")\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\",system_prompt),\n",
    "        (\"human\",\"{input}\")\n",
    "    ])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "# chains\n",
    "question_answer_chain = create_stuff_documents_chain(llm, prompt)\n",
    "# here prompt requires context in system-prompt create_stuff_doc_chain combines all the docs and sends it to llm\n",
    "\n",
    "rag_chain = create_retrieval_chain(retriever, question_answer_chain)\n",
    "\n",
    "# this retrieval chain retrieves docs and then passes question_ans_chain-- here docs are context and prompt sent to llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I don't know.\""
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# output from llm \n",
    "response = rag_chain.invoke({\"input\": \"What is LLM?\"})\n",
    "response [\"answer\"]\n",
    "# here given webpage doesn't contain anything about llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input': 'what is tf-idf',\n",
       " 'context': [Document(metadata={'description': 'Natural Language Processing (NLP) has become an integral part of various industries, including healthcare, finance, and e-commerce, to name a few. NLP is a subfield of artificial intelligence (AI)…', 'language': 'en', 'source': 'https://medium.com/@prateekgaurav/nlp-zero-to-hero-part-1-introduction-bow-tf-idf-word2vec-c1b11ed77a2#:~:text=Word2Vec%20vs.,in%20a%20dense%20vector%20space.', 'title': 'NLP: Zero To Hero [Part 1: Introduction, BOW, TF-IDF & Word2Vec] | by Prateek Gaurav | Medium'}, page_content=\"NLP: Zero To Hero [Part 1: Introduction, BOW, TF-IDF & Word2Vec] | by Prateek Gaurav | MediumOpen in appSign upSign inWriteSign upSign inNLP: Zero To Hero [Part 1: Introduction, BOW, TF-IDF & Word2Vec]Prateek Gaurav·Follow10 min read·Mar 23, 2023--1ListenShareLink to Part 2 of this article:NLP: Zero To Hero [Part 2: Vanilla RNN, LSTM, GRU & Bi-Directional LSTM]Link to Part 3 of this article:NLP: Zero To Hero [Part 3: Transformer-Based Models & Conclusion]Link to the Colab File:https://github.com/PrateekCoder/NLP_Zero_To_HeroNatural Language Processing (NLP) has become an integral part of various industries, including healthcare, finance, and e-commerce, to name a few. NLP is a subfield of artificial intelligence (AI) that deals with the interaction between computers and humans in natural language. With the rise of big data and the need to extract meaningful insights from unstructured data, NLP has gained a lot of attention in recent years.In this series of articles, we will take a deep dive into NLP and cover everything from pre-processing tasks to advanced models like recurrent neural networks (RNNs), long short-term memory (LSTM), and transformer-based models, the focus will be more on practical implementation, evaluation and comparing the models. We will build a sentiment analysis model using different NLP techniques and evaluate their performance. Whether you are new to NLP or looking to enhance your skills, this series will provide you with a comprehensive understanding of NLP and equip you with the knowledge to apply it to real-world problems.Before we dive into the various NLP techniques, it’s important to understand the initial steps involved in NLP tasks. The first step is to acquire the textual data, which can be sourced from APIs, web scraping, or databases, among other methods. Once we have the data, we need to perform text pre-processing to ensure that the data is clean and ready for analysis. In this article, we will cover various pre-processing techniques, including tokenization, stop-word removal, stemming, and lemmatization. Once the data is pre-processed, we will compare and contrast several vectorization methods, such as Bag-of-Words (BoW), Term Frequency-Inverse Document Frequency (TF-IDF), and Word2Vec. Finally, we will delve into several advanced NLP models, such as RNNs, LSTMs, GRUs, and transformers, and build a sentiment analysis model using these techniques. By the end of this series, you’ll have a solid understanding of NLP and the tools to create robust NLP models for real-world applications.Let’s compare each model/technique with its predecessor, highlighting their advantages and limitations:Bag of Words (BoW) vs. TF-IDF: BoW simply counts the occurrences of words in a document, whereas TF-IDF assigns a weight to each word based on its importance in the document and the entire corpus. TF-IDF overcomes the limitation of BoW, which is assigning equal importance to all words. By giving higher weight to rare words, TF-IDF can better capture the meaning of a document. However, both methods ignore word order and context, resulting in a loss of semantic information.Word2Vec vs. BoW and TF-IDF: Word2Vec is a neural network-based technique that learns continuous word embeddings, capturing the semantic relationships between words. It overcomes the limitations of BoW and TF-IDF by preserving contextual information and representing words in a dense vector space. Word2Vec embeddings can capture semantic relationships like synonyms, antonyms, and analogies. However, Word2Vec has its limitations: it does not handle out-of-vocabulary (OOV) words well, and it cannot capture different meanings of a word based on context (polysemy).RNN (including LSTM and GRU) vs. Word2Vec: While Word2Vec learns word representations, RNNs (Recurrent Neural Networks) are used for modeling sequences of data, including text. RNNs can process input sequences of varying lengths, maintaining a hidden state that captures information from previous time steps. Compared to Word2Vec, RNNs can model the temporal dependencies in text, making them suitable for tasks like sentiment analysis or machine translation.LSTM and GRU vs. Vanilla RNN: However, vanilla RNNs suffer from the vanishing gradient problem, making it difficult to learn long-range dependencies. LSTM (Long Short-Term Memory) and GRU (Gated Recurrent Unit) are RNN variants designed to overcome this limitation by incorporating gating mechanisms that allow them to capture long-term dependencies more effectively. Still, RNNs (including LSTMs and GRUs) can be computationally expensive, especially for long sequences.Bi-directional LSTM vs. LSTM: Bi-directional LSTMs are an extension of LSTMs that process the input sequence in both forward and backward directions. This allows the model to capture information from both past and future contexts, often leading to better performance in tasks like named entity recognition and sentiment analysis. However, bi-directional LSTMs are more computationally expensive than regular LSTMs due to the additional backward pass.Transformer vs. RNN (LSTM, GRU): Transformers are a type of neural network architecture that relies on self-attention mechanisms to model the dependencies between words in a sequence. Unlike RNNs, transformers can process input sequences in parallel, making them more computationally efficient, especially for long sequences. Transformers can also capture long-range dependencies more effectively, as they do not have the same constraints as RNNs regarding sequence length. However, transformers can be memory-intensive due to the self-attention mechanism, and they often require large amounts of training data to perform well.By learning these models and techniques, you’ll gain a solid understanding of various NLP approaches and their strengths and weaknesses, which will enable you to choose the best method for a given task.Step 01: Business ProblemFor this article, our business problem is to classify the sentiments of IMDB reviews.Step 02: Collecting the DataWe will be getting our data from Kaggle, the dataset I am using is in CSV format and has already been split into train, test, and validation. You can use the following dataset split the data using train test split. Here is the link to the dataset:https://www.kaggle.com/datasets/columbine/imdb-dataset-sentiment-analysis-in-csv-formatStep 03: Text Pre-ProcessingText pre-processing is an essential step in natural language processing (NLP) that involves cleaning and preparing textual data before it can be used for analysis or modeling. Text pre-processing typically involves several steps, which may vary depending on the specific task and dataset.Special characters removal: The first step in text pre-processing is typically to remove any special characters, punctuation, or other non-alphanumeric characters from the text. This step is important to ensure that the text is clean and standardized, which can make it easier to analyze and model.Lowercasing: After removing special characters, the next step is often to convert all the text to lowercase. This is done to ensure that words with different capitalization are treated as the same, which can improve the accuracy of downstream tasks like classification or clustering.Tokenization: Once the text has been cleaned and standardized, the next step is to split it into individual tokens or words. This is typically done using a tokenizer, which can split the text based on spaces, punctuation, or other delimiters.Stop words removal: Stop words are common words that do not carry much meaning in a sentence, such as “the”, “a”, or “an”. Removing stop words can reduce the dimensionality of the text data and improve the performance of downstream tasks like classification or clustering.Stemming or lemmatization: The final step in text pre-processing is typically to apply a stemming or lemmatization algorithm to reduce words to their base form. Stemming involves removing the suffixes from words to create a stem, while lemmatization involves mapping words to their root form based on their part of speech. The goal of stemming or lemmatization is to reduce the variability in the text data and improve the accuracy of downstream tasks.When it comes to choosing between stemming and lemmatization, there are a few factors to consider. Stemming is typically faster and more computationally efficient than lemmatization, but it can sometimes produce less accurate results since it does not take into account the context of the word. Lemmatization, on the other hand, is slower and more computationally intensive, but it can produce more accurate results since it maps words to their dictionary form.Overall, the choice between stemming and lemmatization depends on the specific task and dataset, and there is no one-size-fits-all approach. It’s often a good idea to experiment with both techniques and evaluate their performance on your specific task before making a final decision.Here is the video and the code to perform text pre-processing on our IMDB dataset.Step 04: Vectorization or Feature ExtractionVectorization, also known as feature extraction, is a critical step in natural language processing (NLP) that involves converting textual data into numerical vectors that can be used by machine learning algorithms. There are several popular methods for vectorizing textual data, including bag-of-words (BoW), term frequency-inverse document frequency (TF-IDF), and word2vec.The boW is a simple and widely used method for vectorizing text data, which involves creating a dictionary of all the unique words in a corpus and counting the number of times each word appears in a document. This results in a vector representation of the document, where each element represents the count of a particular word. The boW is often used in text classification tasks and can be a good starting point for vectorizing text data.TF-IDF is a more advanced method for vectorizing text data that considers the importance of each word in a document and across a corpus. TF-IDF involves computing a weight for each word in a document based on its frequency in the document and its rarity across the corpus. This can help to give more weight to words that are important for distinguishing between different documents and can improve the accuracy of downstream tasks like clustering or retrieval.Word2vec is a neural network-based method for vectorizing text data that learns dense, low-dimensional embeddings of words based on their co-occurrence patterns in a corpus. Word2vec can be trained using either the continuous bag-of-words (CBOW) or skip-gram architectures, which involve predicting the context words given a target word or vice versa. Word2vec can be used to capture semantic relationships between words, such as synonyms or analogies, and can be a powerful tool for tasks like sentiment analysis or information retrieval.Overall, the choice of vectorization method depends on the specific task and dataset, and there is no one-size-fits-all approach. It’s often a good idea to experiment with multiple methods and evaluate their performance on your specific task before making a final decision.Step 05: Building Models For Sentiment AnalysisSVM with Bag Of Words VectorizerSentiment Analysis Model using SVM and BOW VectorizerAccuracy of SVM and BOW Vectorizer ModelAccuracy of SVM and BOW Vectorizer Model: 88.04%The whole process took about 50 minutes, from vectorization to fitting and predicting but gave a very good accuracy of 88%.SVM with Tf-IDF VectorizerSentiment Analysis Model using SVM and Tf-IDF VectorizerAccuracy of SVM and Tf-IDF Vectorizer ModelAccuracy of SVM and Tf-IDF Vectorizer Model: 90.04%Using SVM and Tf-IDF, the process again took over an hour for feature extraction, model fitting, and prediction but the accuracy was really good at 90%.SVM with Custom Word2Vec VectorizerSentiment Analysis Model using SVM and Custom Word2Vec VectorizerAccuracy of SVM and Custom Word2Vec Vectorizer ModelAccuracy of SVM and Custom Word2Vec Vectorizer Model : 49.92%I tried to build a custom word2vec model here, building a custom word2vec which is a neural network-based model that requires a lot of data for training, computation resource, and experimentation with hyperparameters. If you want to optimize this model, you can try a few other things like:Increase the vector size: By increasing the vector size to a larger number (e.g., 300 or 500), you can capture more information about each word and potentially improve the quality of your embeddings.Use a pre-trained word2vec model: Instead of training your own word2vec model from scratch, you can use a pre-trained model that has been trained on a large corpus of text, such as Google’s pre-trained word2vec model or Stanford’s GloVe model. These pre-trained models have been shown to produce high-quality embeddings and can be fine-tuned for your specific task.Use a better classification model: SVM may not be the best choice for sentiment analysis, especially when working with high-dimensional embeddings. You can try using other classifiers such as logistic regression, random forests, or neural networks.Use a larger dataset: If your dataset is small, it may not be representative of the full range of sentiment expressions that you are interested in. Using a larger dataset can help improve the quality of your model.Preprocess your data: While using raw text data may work well for RNN models, preprocessing your data (e.g., removing stop words, stemming/lemmatizing, etc.) may still improve the quality of your word2vec embeddings and your sentiment analysis model.Use cross-validation: To get a better estimate of the true accuracy of your model, you can use k-fold cross-validation instead of just training on a single train-test split. This can help you better evaluate your model’s performance and tune your hyperparameters.Or, you can use pre-trained word2vec models like Glove or Google News 300 word2vec model. I am going to try again with the google news word2vec model.SVM with Google News 300 Word2Vec VectorizerSentiment Analysis Model using SVM and Google News 300 Word2Vec VectorizerAccuracy of SVM and Google News 300 Word2Vec Vectorizer ModelAccuracy of SVM and Google News 300 Word2Vec Vectorizer Model : 85.74%As you can see when we used a pre-trained word2vec model, our accuracy improved from 49% to ~86% which is a drastic improvement, but it is still not better than SVM with BOW or Tf-IDF contrary to my expectation.This basically means we should not always be going with the most complex models available, it's always better to start with the simple models.In the next parts of this article, I will be building sentiment analysis models using Recurrent Neural Networks like Vanilla RNN, LSTM, GRU, Bi-Directional LSTM, and Transformer based models like Distilbert and Roberta and compare their performance.Here is the link to NLP: Zero To Hero [Part 2: Vanilla RNN, LSTM, GRU & Bi-Directional LSTM].Natural Language ProcessSentiment AnalysisRecurrent Neural Network----1FollowWritten by Prateek Gaurav449 FollowersSr. DS Manager @ LGE | Ex - Amazon Data Scientist | Data Science Mentor | Boston University Graduate www.letsdatascience.comFollowHelpStatusAboutCareersPressBlogPrivacyTermsText to speechTeams\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\"),\n",
       "  Document(metadata={'source': 'https://medium.com/@prateekgaurav/nlp-zero-to-hero-part-1-introduction-bow-tf-idf-word2vec-c1b11ed77a2#:~:text=Word2Vec%20vs.,in%20a%20dense%20vector%20space.'}, page_content=''),\n",
       "  Document(metadata={'source': 'https://medium.com/@prateekgaurav/nlp-zero-to-hero-part-1-introduction-bow-tf-idf-word2vec-c1b11ed77a2#:~:text=Word2Vec%20vs.,in%20a%20dense%20vector%20space.'}, page_content=''),\n",
       "  Document(metadata={'source': 'https://medium.com/@prateekgaurav/nlp-zero-to-hero-part-1-introduction-bow-tf-idf-word2vec-c1b11ed77a2#:~:text=Word2Vec%20vs.,in%20a%20dense%20vector%20space.'}, page_content='')],\n",
       " 'answer': \"TF-IDF (Term Frequency-Inverse Document Frequency) is a numerical statistic that is used to scale the importance of words in a document or a corpus. It is a widely used technique in natural language processing (NLP) and information retrieval to weight terms based on their rarity across the entire corpus.\\n\\nTF-IDF takes into account two factors:\\n\\n1. Term Frequency (TF): This measures how frequently a word appears in a particular document. The more frequently a word appears, the higher its TF value.\\n2. Inverse Document Frequency (IDF): This measures how rare a word is across the entire corpus. The more rare a word is, the higher its IDF value.\\n\\nThe TF-IDF value for a word in a document is calculated by multiplying its TF value by its IDF value. This results in a weighted score that represents the importance of the word in the document.\\n\\nHere's the formula:\\n\\nTF-IDF = TF \\\\* IDF\\n\\nWhere:\\n\\n* TF is the term frequency, calculated as the number of times the word appears in the document divided by the total number of words in the document.\\n* IDF is the inverse document frequency, calculated as the natural logarithm of the total number of documents divided by the number of documents that contain the word.\\n\\nThe IDF value is calculated as:\\n\\nIDF = log(N / df)\\n\\nWhere:\\n\\n* N is the total number of documents in the corpus.\\n* df is the number of documents that contain the word.\\n\\nBy using TF-IDF, you can:\\n\\n1. Reduce the impact of common words (such as stopwords) that do not carry much meaning.\\n2. Increase the impact of rare words that are more specific to the document or topic.\\n3. Improve the accuracy of text classification, clustering, and retrieval models.\\n\\nIn the context of the article, TF-IDF is used as a vectorization method to convert text documents into numerical vectors that can be used by machine learning algorithms for sentiment analysis.\"}"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# given web page only contains info about embeddings\n",
    "response_1 = rag_chain.invoke({\"input\":\"what is tf-idf\"})\n",
    "response_1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ADDING CHAT HISTORY\n",
    "- we need to create a model such that it remembers previously asked questions\n",
    "- example ::Human: \"What is Tf-idf?\"---> AI: gives some answer-->human: tell me more about IT--> here it represents tf-idf and model need to capture this\n",
    "- initially we have [query -> retriever] now [(query, conversation history) -> LLM -> rephrased query -> retriever]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we need to update existing app with 1. prompt and 2. contextualizing questions\n",
    "\n",
    "from langchain.chains import create_history_aware_retriever # Create a chain that takes conversation history and returns documents.\n",
    "from langchain_core.prompts import MessagesPlaceholder # to pass a list of messages to prompt using chat history input\n",
    "\n",
    "contextualize_q_system_prompt = (\n",
    "    \"Given a chat history and the latest user question \"\n",
    "    \"which might reference context in the chat history, \"\n",
    "    \"formulate a standalone question which can be understood \"\n",
    "    \"without the chat history. Do NOT answer the question, \"\n",
    "    \"just reformulate it if needed and otherwise return it as is.\"\n",
    ")\n",
    "\n",
    "contextualize_q_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", contextualize_q_system_prompt),\n",
    "        MessagesPlaceholder(\"chat_history\"), # here variable chat_history is used to store all the messages history\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "# here we are upgrading our retriever to his_aware_Ret where we get op from original retriever considering the input and chat_history as inputs (chat history given in prompt)\n",
    "history_aware_retriever = create_history_aware_retriever(\n",
    "    llm, retriever, contextualize_q_prompt\n",
    ")\n",
    "\n",
    "# using previous system prompt which takes context from webpage\n",
    "qa_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system_prompt),\n",
    "        MessagesPlaceholder(\"chat_history\"),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# here to generate q/a(question_ans_chain) with input keys context,message_history,input_query\n",
    "question_answer_chain = create_stuff_documents_chain(llm, qa_prompt)\n",
    "\n",
    "# this chain applies history_aware_retriever and question_answer_chain in sequence\n",
    "rag_chain = create_retrieval_chain(history_aware_retriever, question_answer_chain)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bag-of-Words (BoW) and Term Frequency-Inverse Document Frequency (TF-IDF) are two popular techniques used in natural language processing (NLP) for feature extraction from text data. While both techniques are used to convert text data into numerical vectors, they differ in their approach and assumptions.\n",
      "\n",
      "**Bag-of-Words (BoW)**\n",
      "\n",
      "BoW is a simple and widely used technique that represents a document as a bag, or a set, of its word frequencies. BoW assumes that the order of words in a document is irrelevant and that the frequency of each word is sufficient to capture the document's meaning.\n",
      "\n",
      "Here are the key characteristics of BoW:\n",
      "\n",
      "* **Simple**: BoW is a straightforward and easy-to-implement technique.\n",
      "* **Frequency-based**: BoW focuses solely on the frequency of each word in a document.\n",
      "* **Lack of context**: BoW ignores the context in which words appear in a document.\n",
      "* **No consideration of rarity**: BoW does not consider the rarity of words across the corpus.\n",
      "\n",
      "**Term Frequency-Inverse Document Frequency (TF-IDF)**\n",
      "\n",
      "TF-IDF is a more advanced technique that builds upon the BoW approach by considering the rarity of words across the corpus. TF-IDF takes into account both the frequency of each word in a document (Term Frequency) and its rarity across the corpus (Inverse Document Frequency).\n",
      "\n",
      "Here are the key characteristics of TF-IDF:\n",
      "\n",
      "* **More nuanced**: TF-IDF is a more sophisticated technique that captures both local (word frequency) and global (word rarity) information.\n",
      "* **Context-aware**: TF-IDF considers the context in which words appear in a document, as it takes into account the frequency of words across the corpus.\n",
      "* **Weighting**: TF-IDF assigns a weight to each word based on its importance, which is determined by its frequency and rarity.\n",
      "\n",
      "**Key differences**\n",
      "\n",
      "Here are the main differences between BoW and TF-IDF:\n",
      "\n",
      "* **Context**: BoW ignores context, while TF-IDF considers the context in which words appear in a document.\n",
      "* **Rarity**: BoW does not consider the rarity of words across the corpus, while TF-IDF does.\n",
      "* **Weighting**: BoW uses a simple frequency-based weighting scheme, while TF-IDF uses a more complex weighting scheme that takes into account the rarity of words.\n",
      "* **Complexity**: BoW is a simpler technique, while TF-IDF is more complex and computationally expensive.\n",
      "\n",
      "In general, TF-IDF is a more effective technique for text classification, information retrieval, and topic modeling tasks, as it captures both local and global information about the text. However, BoW can still be useful in certain situations, such as when the focus is on the frequency of specific words or when the corpus is small and TF-IDF is computationally expensive.\n"
     ]
    }
   ],
   "source": [
    "# as we have defined variable chat_history we need to store all the messages and give it to chat_history\n",
    "from langchain_core.messages import AIMessage,HumanMessage\n",
    "\n",
    "# list to store chat history from model\n",
    "chat_history = []\n",
    "\n",
    "first_question = \"what is tf-idf?\"\n",
    "ai_msg_1 = rag_chain.invoke({\"input\": first_question, \"chat_history\": chat_history})\n",
    "\n",
    "# to save the outputs in list of chat_history\n",
    "chat_history.extend(\n",
    "    [\n",
    "        HumanMessage(content=first_question),\n",
    "        AIMessage(content=ai_msg_1[\"answer\"]),\n",
    "    ]\n",
    ")\n",
    "\n",
    "second_question = \"What are differences between it and Bow?\"\n",
    "ai_msg_2 = rag_chain.invoke({\"input\": second_question, \"chat_history\": chat_history})\n",
    "\n",
    "print(ai_msg_2[\"answer\"])\n",
    "# model is able to capture it stands for tf-idf in second_question"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
